# IA Modules

**Production-ready AI agent framework with enterprise-grade reliability and observability**

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![Tests](https://github.com/yourusername/ia_modules/actions/workflows/test.yml/badge.svg)](https://github.com/yourusername/ia_modules/actions/workflows/test.yml)
[![Coverage](https://img.shields.io/badge/coverage-49%25-yellow.svg)](htmlcov/index.html)
[![EARF Compliant](https://img.shields.io/badge/EARF-compliant-success.svg)](docs/RELIABILITY_USAGE_GUIDE.md)

Build reliable, observable, and verifiable AI agent systems with graph-based pipelines, comprehensive reliability metrics, and enterprise-grade features.

## Table of Contents

- [Overview](#overview)

- [Pipeline Examples & Features](docs/TEST_PIPELINES_GUIDE.md) **See framework capabilities in action with runnable examples**
- [Core Features](#core-features)
- [Quickstart: Your First Pipeline](#quickstart-your-first-pipeline)
- [Core Architecture Principles](#core-architecture-principles)
- [Package Structure](#package-structure)
- [Key Components](#key-components)
  - [Pipeline System](#pipeline-system)
  - [Authentication System](#authentication-system)
  - [Database Layer](#database-layer)
- [Defining Pipelines (JSON Format)](#defining-pipelines-json-format)
  - [Top-Level Fields](#top-level-fields)
  - [Step Definition](#step-definition)
  - [Flow & Routing](#flow--routing)
  - [Templating and Parameterization](#templating-and-parameterization)
  - [Full JSON Example](#full-json-example)
- [Running Pipelines](#running-pipelines)
  - [Application Responsibilities](#application-responsibilities)
  - [Option 1: DB-Backed Execution (Production)](#option-1-db-backed-execution-production)
  - [Option 2: File-Based Execution (Development)](#option-2-file-based-execution-development)
- [AI/LLM Integration](#aillm-integration)
- [Human-in-the-Loop (HITL)](#human-in-the-loop-hitl)
- [Parallel Processing](#parallel-processing)
- [Advanced Topics](#advanced-topics)
  - [The Pipeline Importer Service](#the-pipeline-importer-service)
  - [Database Schema](#database-schema)
- [Installation](#installation)
- [Contributing](#contributing)

## What's New in v0.0.3 üéâ

**Enterprise Agent Reliability Framework (EARF) Compliance**

- ‚úÖ **Reliability Metrics**: Track SR, CR, PC, HIR, MA, TCL, WCT across all workflows
- ‚úÖ **Multiple Storage Backends**: PostgreSQL, MySQL, SQLite, Redis, In-Memory
- ‚úÖ **SLO Monitoring**: Define and monitor Service Level Objectives
- ‚úÖ **Event Replay**: Debug production failures by replaying step executions
- ‚úÖ **Auto-Evidence Collection**: Automatic compliance evidence capture
- ‚úÖ **644/650 Tests Passing**: 99.1% test coverage across 13 reliability modules
- ‚úÖ **Python 3.13 Compatible**: All datetime deprecations fixed

See [MIGRATION.md](MIGRATION.md) for upgrade guide (100% backward compatible).

## Overview

IA Modules provides a comprehensive framework for building production-ready AI agent systems with enterprise-grade reliability and observability. Build complex workflows as graphs, track reliability metrics in real-time, and ensure compliance with automated evidence collection.

**Perfect for:**
- AI agent systems requiring reliability guarantees
- Multi-step workflows with complex dependencies
- Production systems needing observability and compliance
- Teams building on LangChain/LangGraph seeking better reliability

## Core Features

### üöÄ **Pipeline & Workflow**
- **Graph-Based Execution**: Define workflows as directed graphs (including cycles)
- **Conditional Routing**: Dynamic routing based on execution results
- **Parallel Execution**: Automatic concurrent execution of independent steps
- **Checkpointing**: Resume failed pipelines from last successful step
- **Human-in-the-Loop**: Pause workflows for human review and approval

### üìä **Reliability & Observability (EARF)**
- **Comprehensive Metrics**: SR, CR, PC, HIR, MA, TCL, WCT tracking
- **Multiple Storage**: PostgreSQL, MySQL, SQLite, Redis, In-Memory
- **SLO Monitoring**: Real-time SLO compliance monitoring
- **Event Replay**: Debug failures by replaying step executions
- **Auto-Evidence**: Automated compliance evidence collection

### ü§ñ **AI Agent Features**
- **Multi-Agent Orchestration**: Coordinate multiple AI agents
- **LLM Integration**: Google Gemini, OpenAI, Anthropic support
- **Grounding & Validation**: Schema validation and fact verification
- **Memory Management**: Conversation history and context
- **Citation Tracking**: Track sources for agent outputs

### üõ†Ô∏è **Developer Tools**
- **CLI Tool**: Validate, run, visualize pipelines from command line
- **Benchmarking**: Compare pipeline performance and accuracy
- **Plugin System**: Extend with custom steps and storage backends
- **Type Safety**: Pydantic-based schema validation

## Quick Start

### Option 1: Try the Showcase App (Recommended)

The fastest way to see IA Modules in action:

```bash
# Clone the repository
git clone <repository-url>
cd ia_modules

# Install everything (framework + showcase app)
./install.sh     # Linux/Mac
# or
install.bat      # Windows

# Start the showcase app
./start.sh       # Linux/Mac
# or
start.bat        # Windows
```

Open http://localhost:3000 and explore:
- üöÄ Example pipelines you can run
- üìä Real-time reliability metrics
- üéØ SLO compliance monitoring
- üíª Interactive execution monitoring

### Option 2: Package Installation

```bash
# Basic installation
pip install ia_modules

# With reliability SQL backends
pip install ia_modules[sql]

# With Redis support
pip install ia_modules[redis]

# Everything (recommended for production)
pip install ia_modules[all]
```

### Your First Pipeline with Reliability Metrics

```python
from ia_modules.pipeline.core import PipelineStep, StepResult
from ia_modules.pipeline.runner import PipelineRunner
from ia_modules.reliability.metrics import ReliabilityMetrics
from ia_modules.reliability.memory_storage import InMemoryMetricStorage

# Define a simple step
class ProcessDataStep(PipelineStep):
    async def execute(self, context):
        data = context.get_data("input")
        result = data.upper()
        return StepResult(
            success=True,
            data={"output": result},
            next_step="end"
        )

# Set up reliability tracking
async def main():
    storage = InMemoryMetricStorage()
    metrics = ReliabilityMetrics(storage)

    runner = PipelineRunner(metrics=metrics)
    runner.register_step("process", ProcessDataStep())

    # Run pipeline
    result = await runner.run(
        start_step="process",
        initial_data={"input": "hello world"}
    )

    # Get reliability report
    report = await metrics.get_report()
    print(f"Success Rate: {report.sr:.2%}")
    print(f"Compensation Rate: {report.cr:.2%}")

import asyncio
asyncio.run(main())
```

## Documentation

### üìö **Essential Guides**
- **[Getting Started](docs/GETTING_STARTED.md)** ‚≠ê Start here! 5-minute quickstart
- **[Features Overview](docs/FEATURES.md)** - Complete feature matrix
- **[Migration Guide](MIGRATION.md)** - Upgrade from v0.0.2 (100% compatible)
- **[API Reference](docs/API_REFERENCE.md)** - Detailed API documentation

### üîß **Production Deployment**
- **[Reliability Usage Guide](docs/RELIABILITY_USAGE_GUIDE.md)** - EARF compliance and monitoring
- **[CLI Documentation](docs/CLI_TOOL_DOCUMENTATION.md)** - Command-line tools
- **[Plugin System](docs/PLUGIN_SYSTEM_DOCUMENTATION.md)** - Extending IA Modules

### üèóÔ∏è **Architecture & Development**
- **[Pipeline Architecture](docs/PIPELINE_ARCHITECTURE.md)** - Core system design
- **[Developer Guide](docs/DEVELOPER_GUIDE.md)** - Development workflow
- **[Contributing Guide](CONTRIBUTING.md)** - How to contribute
- **[Testing Guide](docs/TESTING_GUIDE.md)** - Testing strategies

## Quickstart: Your First Pipeline

Let's create and run a simple two-step pipeline.

1. Create your project structure:

```bash
mkdir my_pipeline_app
cd my_pipeline_app
mkdir pipelines
touch main.py
touch steps.py
```

2. Define the pipeline (`pipelines/hello_world.json`):

```json
{
  "name": "Hello World Pipeline",
  "version": "1.0",
  "steps": [
    {
      "id": "step_one",
      "step_class": "GenerateMessageStep",
      "module": "steps"
    },
    {
      "id": "step_two",
      "step_class": "PrintMessageStep",
      "module": "steps",
      "inputs": {
        "message": "{{ step_one.greeting }}"
      }
    }
  ],
  "flow": {
    "start_at": "step_one",
    "transitions": [
      { "from": "step_one", "to": "step_two" }
    ]
  }
}
```

3. Implement the steps (`steps.py`):

```python
# steps.py
from ia_modules.pipeline import Step

class GenerateMessageStep(Step):
    async def run(self, data):
        name = data.get("name", "World")
        return {"greeting": f"Hello, {name}!"}

class PrintMessageStep(Step):
    async def run(self, data):
        message = data.get("message")
        print(message)
        return {"status": "Message printed successfully"}
```

4. Create the runner (`main.py`):

```python
# main.py
import asyncio
from ia_modules.pipeline.runner import run_pipeline_from_json

async def main():
    print("Running the pipeline...")
    result = await run_pipeline_from_json(
        pipeline_file="pipelines/hello_world.json",
        input_data={"name": "Developer"}
    )
    print("\nPipeline finished with result:")
    print(result)

if __name__ == "__main__":
    asyncio.run(main())
```

5. Run it!

Make sure `ia_modules` is installed and in your `PYTHONPATH`.

```bash
python main.py
```

Expected Output:

```
Running the pipeline...
Hello, Developer!

Pipeline finished with result:
{'status': 'Message printed successfully'}
```

## Core Architecture Principles

1. **Graph-First Design**: All pipelines are defined as **directed acyclic graphs (DAGs)**, ensuring a clear, predictable, and finite execution flow.
2. **Intelligent Flow Control**:
   - **Expression-based routing**: Simple logical conditions on step outputs (e.g., `result.score > 0.8`).
   - **AI-driven routing**: (Future) Use an agent to make complex routing decisions.
   - **Function-based routing**: (Future) Define routing logic with custom Python functions.
3. **Service Injection & Dependency Management**: A clean `ServiceRegistry` pattern provides steps with managed access to shared resources like database connections and HTTP clients.

## Package Structure

```
ia_modules/
‚îú‚îÄ‚îÄ pipeline/            # Core pipeline execution engine
‚îú‚îÄ‚îÄ auth/                # Authentication and session management
‚îú‚îÄ‚îÄ database/            # Database abstraction and management
‚îú‚îÄ‚îÄ web/                 # Web utilities (execution tracking, etc.)
‚îî‚îÄ‚îÄ data/                # Shared data models
```

## Key Components

#### Pipeline System

- **Step**: The base class for all pipeline steps. Implements the `work` method containing business logic.
- **Pipeline**: The orchestrator that executes the graph, manages state, and handles routing.
- **HumanInputStep**: A specialized step that pauses execution to wait for human interaction.
- **ServiceRegistry**: A dependency injection container for services (DB, HTTP, etc.).

#### Authentication System

- **AuthMiddleware**: FastAPI-compatible middleware for protecting endpoints.
- **SessionManager**: Manages secure user sessions.

#### Database Layer

- **DatabaseManager**: Handles connections to multiple database backends (e.g., SQLite, PostgreSQL).
- **Migration System**: Manages database schema versioning and updates.

## Defining Pipelines (JSON Format)

Pipelines are defined as JSON documents, making them language-agnostic and easy to store, version, and edit.

### Top-Level Fields

- `name` (string): Human-readable name for the pipeline.
- `version` (string): Version of the pipeline (e.g., "1.0").
- `parameters` (object, optional): Default input parameters for a pipeline run.
- `steps` (array): A list of all step objects in the graph.
- `flow` (object): Defines the execution order and conditional transitions.
- `error_handling` (object, optional): Global configuration for retries, timeouts, etc.
- `outputs` (object, optional): Defines the final output of the entire pipeline, often templated from step results.

### Step Definition

- `id` (string): A unique identifier for the step within the pipeline.
- `step_class` (string): The Python class name that implements the step's logic.
- `module` (string): The Python module path where the `step_class` can be found.
- `inputs` (object, optional): Maps required inputs for the step to outputs from other steps or pipeline parameters.
- `config` (object, optional): Static configuration passed to the step instance.

### Flow & Routing

- `flow.start_at` (string): The `id` of the first step to execute.
- `flow.transitions` (array): A list of directed edges in the graph. Each transition includes:
  - `from` (string): The source step `id`.
  - `to` (string): The destination step `id`.
  - `condition` (string | object, optional): The routing rule. Defaults to `"always"`.
    - **Simple Conditions**: `"always"` or `"parameter:my_param"` (checks for truthiness of a pipeline parameter).
    - **Expression Conditions**: An object for more complex logic, e.g., `{"type": "expression", "config": {"source": "step_id.output_name", "operator": "gt", "value": 100}}`.

### Templating and Parameterization

The framework uses a simple templating syntax to pass data between steps and from pipeline parameters.

- **Reference a step's output**: `{{ step_id.output_name }}`
- **Reference a pipeline parameter**: `{{ parameters.param_name }}`

This syntax is used within the `inputs` mapping of a step.

> Note: Consolidating to `{{ ... }}` is recommended for clarity. If backwards compatibility is required for the single-brace style, document it as deprecated.

### Full JSON Example

This pipeline scrapes a Wikipedia page and processes its content.

```json
{
  "name": "AI Notebook Creation Pipeline",
  "description": "Creates research notebooks with AI-enhanced content.",
  "version": "1.0",
  "parameters": {
    "topic": "artificial intelligence",
    "enable_ai_processing": true
  },
  "steps": [
    {
      "id": "wikipedia_scraper",
      "name": "Wikipedia Content Scraper",
      "step_class": "WikipediaScraperStep",
      "module": "knowledge_processing.wikipedia_scraper_step",
      "inputs": {
        "topic": "{{ parameters.topic }}"
      },
      "outputs": ["scraped_content", "article_title"]
    },
    {
      "id": "structured_processor",
      "name": "Structured Content Processor",
      "step_class": "StructuredWikipediaProcessorStep",
      "module": "knowledge_processing.structured_wikipedia_processor_step",
      "inputs": {
        "scraped_html": "{{ wikipedia_scraper.scraped_content }}",
        "title": "{{ wikipedia_scraper.article_title }}"
      },
      "outputs": ["structured_content"]
    }
  ],
  "flow": {
    "start_at": "wikipedia_scraper",
    "transitions": [
      {
        "from": "wikipedia_scraper",
        "to": "structured_processor"
      }
    ]
  }
}
```

## Running Pipelines

There are two primary ways to execute a pipeline: backed by a database (for production) or directly from a file (for development).

### Application Responsibilities

The `ia_modules` library provides the execution engine. The consuming application is responsible for:

1. **Storing Pipeline JSON**: Keeping pipeline definitions in a local directory (e.g., `pipelines/`).
2. **Providing Services**: Creating and configuring the `ServiceRegistry` with application-specific database connections, HTTP clients, and other services.
3. **Database Integration**: (For production) Running the necessary database migrations and using the `PipelineImportService` to load JSON definitions into the database.

### Option 1: DB-Backed Execution (Production)

This is the standard approach for a deployed application.

```python
from ia_modules.pipeline.importer import PipelineImportService
from ia_modules.pipeline.runner import create_pipeline_from_json
from ia_modules.pipeline.services import ServiceRegistry

# 1. Setup application services
services = ServiceRegistry(db=app_db_manager, http=http_client)

# 2. Load pipeline configuration from the database
importer = PipelineImportService(db_provider, pipelines_dir='/path/to/pipelines')
pipeline_row = await importer.get_pipeline_by_slug('ai-notebook-creation-pipeline-v1')
pipeline_config = pipeline_row['pipeline_config'] # This is the parsed JSON

# 3. Create and run the pipeline instance
pipeline = create_pipeline_from_json(pipeline_config, services=services)
result = await pipeline.run({'topic': 'machine learning'})
```

### Option 2: File-Based Execution (Development)

Ideal for local development, testing, and ad-hoc runs.

```python
from ia_modules.pipeline.runner import run_pipeline_from_json
from ia_modules.pipeline.services import ServiceRegistry

# 1. Setup services (can be mocked for testing)
services = ServiceRegistry(db=mock_db, http=mock_http)

# 2. Run directly from the JSON file
result = await run_pipeline_from_json(
    pipeline_file="pipelines/ai_notebook_creation_pipeline.json",
    input_data={"topic": "machine learning"},
    services=services
)
```

## AI/LLM Integration

The framework includes comprehensive support for Large Language Model integration with multiple providers.

### Supported Providers

| Provider | Environment Variable | Models | Features |
|----------|---------------------|---------|----------|
| **Google Gemini** | `GEMINI_API_KEY` | gemini-2.5-flash, gemini-2.5-pro | Fast, cost-effective |
| **OpenAI** | `OPENAI_API_KEY` | gpt-3.5-turbo, gpt-4o | Wide compatibility |
| **Anthropic** | `ANTHROPIC_API_KEY` | claude-3-haiku, claude-3-sonnet | Advanced reasoning |

### LLM-Powered Steps

```python
# steps/ai_steps.py
class AIAnalysisStep(Step):
    async def run(self, data):
        llm_service = self.services.get('llm_provider')

        # Generate structured output with schema validation
        result = await llm_service.generate_structured_output(
            prompt=f"Analyze this data: {data['text']}",
            schema={
                "type": "object",
                "properties": {
                    "sentiment": {"type": "string"},
                    "confidence": {"type": "number"},
                    "key_topics": {"type": "array"}
                }
            }
        )

        return {"analysis": result}
```

### Running AI-Enhanced Pipelines

```bash
# Set API key
export GEMINI_API_KEY="your_api_key_here"

# Run AI-powered pipeline
python tests/pipeline_runner_with_llm.py pipelines/ai_analysis.json --input '{"text": "Your content here"}'
```

## Human-in-the-Loop (HITL)

Pipelines can include a `HumanInputStep` to pause execution and wait for external input. When the runner encounters this step type, it will halt and return a payload indicating that human action is required.

**Example HITL Step Definition:**

```json
{
  "id": "human_approval",
  "type": "human_input",
  "step_class": "HumanInputStep",
  "config": {
    "ui_schema": {
      "title": "Review Content for Approval",
      "fields": [
        {"name": "decision", "type": "radio", "options": ["Approve", "Reject"]},
        {"name": "notes", "type": "textarea", "label": "Reasoning"}
      ]
    }
  }
}
```

The application's UI can use the `ui_schema` to dynamically render a form. Once the user submits the form, the application can resume the pipeline run, providing the user's data as input to the `human_approval` step.

## Parallel Processing

The framework automatically executes steps in parallel when they have no dependencies on each other.

### Automatic Parallelization

```json
{
  "flow": {
    "start_at": "data_loader",
    "transitions": [
      {"from": "data_loader", "to": "processor_1"},
      {"from": "data_loader", "to": "processor_2"}, 
      {"from": "data_loader", "to": "processor_3"},
      {"from": "processor_1", "to": "merger"},
      {"from": "processor_2", "to": "merger"},
      {"from": "processor_3", "to": "merger"}
    ]
  }
}
```

**Execution Flow:**
1. `data_loader` runs first
2. `processor_1`, `processor_2`, `processor_3` run **simultaneously** 
3. `merger` waits for all three processors to complete

### Performance Benefits

- **üöÄ Speed**: CPU-bound tasks utilize multiple cores
- **‚ö° Throughput**: I/O-bound tasks don't block each other  
- **üìà Scalability**: Add parallel paths to increase capacity
- **üîß Efficiency**: Better resource utilization

## Advanced Topics

### The Pipeline Importer Service

In a production environment, pipelines are loaded from the filesystem into a database table for fast and reliable access. The `PipelineImportService` handles this process.

- **Location**: `ia_modules/pipeline/importer.py`
- **Purpose**: Scans a directory for `*.json` files, validates them, and upserts them into the `pipelines` database table.
- **Change Detection**: It computes a hash of the file content to avoid redundant database writes if a pipeline definition hasn't changed.

The consuming application typically calls `importer.import_all_pipelines()` on startup.

### Database Schema

The importer and runner expect a specific database schema for storing pipelines, execution jobs, and logs. The SQL migration scripts are located in `ia_modules/pipeline/migrations/`.

**It is the consuming application's responsibility to run these migrations.**

You can either:
1. Copy the migration files into your application's migration directory.
2. Configure your migration tool (e.g., Alembic) to discover migrations within `ia_modules`.

The core table is `pipelines`, which stores the JSON definition and metadata for each imported pipeline.

## Why IA Modules?

### vs. LangChain/LangGraph

| Feature | IA Modules | LangChain | LangGraph |
|---------|------------|-----------|-----------|
| **Reliability Metrics** | ‚úÖ SR, CR, PC, HIR, MA, TCL, WCT | ‚ùå | ‚ùå |
| **EARF Compliance** | ‚úÖ Full compliance | ‚ùå | ‚ùå |
| **SQL Storage** | ‚úÖ PostgreSQL, MySQL, SQLite | ‚ùå | Partial |
| **Cyclic Graphs** | ‚úÖ With loop detection | ‚ùå | ‚úÖ |
| **Checkpointing** | ‚úÖ Full state snapshots | ‚ùå | ‚úÖ |
| **SLO Monitoring** | ‚úÖ Real-time | ‚ùå | ‚ùå |
| **Event Replay** | ‚úÖ Debug production | ‚ùå | ‚ùå |
| **Benchmarking** | ‚úÖ Built-in framework | ‚ùå | ‚ùå |
| **CLI Tools** | ‚úÖ Full CLI | Partial | ‚ùå |
| **Test Coverage** | 99.1% (644/650) | Varies | Varies |

See [docs/COMPARISON_LANGCHAIN_LANGGRAPH.md](docs/COMPARISON_LANGCHAIN_LANGGRAPH.md) for detailed comparison.

## Production-Ready Features

### Reliability Metrics (EARF)

Track comprehensive reliability metrics across all workflows:

- **SR (Success Rate)**: % of successful executions
- **CR (Compensation Rate)**: % requiring rollback/compensation
- **PC (Pass Confidence)**: Statistical confidence in success rate
- **HIR (Human Intervention Rate)**: % requiring human review
- **MA (Model Accuracy)**: Agent decision accuracy
- **TCL (Tool Call Latency)**: Average tool execution time
- **WCT (Workflow Completion Time)**: End-to-end duration

### Storage Backends

Choose the right storage for your needs:

- **In-Memory**: Fast, perfect for development
- **SQLite**: Simple, file-based persistence
- **PostgreSQL**: Enterprise-grade, production-ready
- **MySQL**: Wide compatibility
- **Redis**: High-performance caching (optional)

### SLO Monitoring

Define and monitor Service Level Objectives:

```python
from ia_modules.reliability.slo_monitor import SLOMonitor

monitor = SLOMonitor(metrics)
compliance = await monitor.check_compliance()

if not compliance.sr_compliant:
    alert(f"SLO violation: SR={compliance.sr_current:.2%}")
```

## Roadmap

See [ROADMAP.md](ROADMAP.md) for complete roadmap.

### v0.0.4 (Next Release)
- Distributed execution across multiple machines
- Streaming pipeline outputs
- Advanced caching strategies
- Workflow templates
- Web dashboard

### v0.1.0
- Kubernetes deployment
- GraphQL API
- Advanced retry strategies
- Cost optimization
- Multi-tenancy

## Contributing

We welcome contributions! See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

**Quick Start:**
```bash
git clone <repository-url>
cd ia_modules
pip install -e ".[dev,all]"
pytest tests/ -v
```

## License

MIT License - see [LICENSE](LICENSE) for details.

## Support

- **Documentation**: [docs/](docs/)
- **Issues**: [GitHub Issues](https://github.com/yourusername/ia_modules/issues)
- **Examples**: [tests/pipelines/](tests/pipelines/)

---

**Built with ‚ù§Ô∏è for production AI systems**
"""Jailbreak detection guardrail."""
from typing import Any, Dict, Optional
import re
from ..base import BaseGuardrail
from ..models import RailResult, RailAction, RailType


class JailbreakDetectionRail(BaseGuardrail):
    """
    Detect jailbreak attempts in user input.

    Jailbreak patterns:
    - Prompt injection attempts
    - Role-playing bypass ("Ignore previous instructions")
    - System prompt leakage attempts
    - Encoding tricks (base64, etc.)
    """

    # Known jailbreak patterns
    JAILBREAK_PATTERNS = [
        r"ignore\s+(previous|all|above)\s+instructions",
        r"you\s+are\s+now\s+in\s+developer\s+mode",
        r"pretend\s+you\s+are",
        r"roleplaying\s+as",
        r"simulate\s+being",
        r"act\s+as\s+if",
        r"system\s+prompt",
        r"show\s+me\s+your\s+(instructions|rules|prompt)",
        r"base64",
        r"rot13",
        r"please\s+forget\s+everything",
        r"disregard\s+safety",
    ]

    def __init__(self, config, llm_client=None):
        """
        Initialize jailbreak detection rail.

        Args:
            config: Rail configuration
            llm_client: Optional LLM for semantic detection
        """
        super().__init__(config)
        self.llm_client = llm_client
        self.use_llm = llm_client is not None

    async def check(self, content: Any, context: Optional[Dict[str, Any]] = None) -> RailResult:
        """Check for jailbreak attempts."""
        if not isinstance(content, str):
            content = str(content)

        content_lower = content.lower()

        # Pattern-based detection
        for pattern in self.JAILBREAK_PATTERNS:
            if re.search(pattern, content_lower, re.IGNORECASE):
                return RailResult(
                    rail_id=self.config.id,
                    rail_type=RailType.INPUT,
                    action=RailAction.BLOCK,
                    original_content=content,
                    triggered=True,
                    reason=f"Jailbreak pattern detected: {pattern}",
                    confidence=0.9
                )

        # LLM-based semantic detection
        if self.use_llm:
            is_jailbreak = await self._llm_detect_jailbreak(content)

            if is_jailbreak:
                return RailResult(
                    rail_id=self.config.id,
                    rail_type=RailType.INPUT,
                    action=RailAction.BLOCK,
                    original_content=content,
                    triggered=True,
                    reason="LLM detected jailbreak attempt",
                    confidence=0.85
                )

        # No jailbreak detected
        return RailResult(
            rail_id=self.config.id,
            rail_type=RailType.INPUT,
            action=RailAction.ALLOW,
            original_content=content,
            triggered=False
        )

    async def _llm_detect_jailbreak(self, content: str) -> bool:
        """Use LLM to detect subtle jailbreak attempts."""
        prompt = f"""Analyze if this user input is attempting to jailbreak, bypass, or manipulate an AI assistant:

User Input: {content}

Is this a jailbreak attempt? Answer with only YES or NO."""

        response = await self.llm_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0,
            max_tokens=10
        )

        answer = response.choices[0].message.content.strip().upper()
        return "YES" in answer
